2025-03-03:04:18:24,036 INFO     [__main__.py:272] Verbosity set to INFO
2025-03-03:04:18:32,585 INFO     [__main__.py:357] Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`
2025-03-03:04:18:32,585 INFO     [__main__.py:369] Selected Tasks: ['arc_challenge', 'arc_easy', 'glue', 'hellaswag', 'lambada_openai', 'piqa', 'sciq', 'winogrande']
2025-03-03:04:18:32,629 INFO     [evaluator.py:152] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2025-03-03:04:18:32,629 INFO     [evaluator.py:189] Initializing hf model, with arguments: {'pretrained': 'state-spaces/mamba2-370m', 'dtype': 'float', 'trust_remote_code': True}
2025-03-03:04:18:32,841 INFO     [huggingface.py:178] Device not specified
2025-03-03:04:18:32,841 INFO     [huggingface.py:179] Cuda Available? True
Traceback (most recent call last):
  File "/home/zhangping/Projects/.envs/zrc/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
             ^^^^^^^^^^^^^^
  File "/home/zhangping/Projects/.envs/zrc/lib/python3.12/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhangping/Projects/.envs/zrc/lib/python3.12/site-packages/lm_eval/utils.py", line 395, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhangping/Projects/.envs/zrc/lib/python3.12/site-packages/lm_eval/evaluator.py", line 192, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhangping/Projects/.envs/zrc/lib/python3.12/site-packages/lm_eval/api/model.py", line 148, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhangping/Projects/.envs/zrc/lib/python3.12/site-packages/lm_eval/models/huggingface.py", line 208, in __init__
    self._create_tokenizer(
  File "/home/zhangping/Projects/.envs/zrc/lib/python3.12/site-packages/lm_eval/models/huggingface.py", line 653, in _create_tokenizer
    self.tokenizer = transformers.AutoTokenizer.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhangping/Projects/.envs/zrc/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 953, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhangping/Projects/.envs/zrc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 2020, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'state-spaces/mamba2-370m'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'state-spaces/mamba2-370m' is the correct path to a directory containing all relevant files for a GPTNeoXTokenizerFast tokenizer.
