{
  "model": "/home/zhangping/zrc/RWKV-x060-World-1B6-v2.1-20240328-ctx4096",
  "tasks": [
    "mmlu"
  ],
  "num_fewshot": 5,
  "results": {
    "mmlu": {
      "acc,none": 0.25986326734083465,
      "acc_stderr,none": 0.0036907603302344947,
      "alias": "mmlu"
    },
    "mmlu_humanities": {
      "alias": " - humanities",
      "acc,none": 0.2507970244420829,
      "acc_stderr,none": 0.006319026810391236
    },
    "mmlu_formal_logic": {
      "alias": "  - formal_logic",
      "acc,none": 0.23809523809523808,
      "acc_stderr,none": 0.03809523809523809
    },
    "mmlu_high_school_european_history": {
      "alias": "  - high_school_european_history",
      "acc,none": 0.23030303030303031,
      "acc_stderr,none": 0.03287666758603482
    },
    "mmlu_high_school_us_history": {
      "alias": "  - high_school_us_history",
      "acc,none": 0.2647058823529412,
      "acc_stderr,none": 0.030964517926923372
    },
    "mmlu_high_school_world_history": {
      "alias": "  - high_school_world_history",
      "acc,none": 0.26582278481012656,
      "acc_stderr,none": 0.02875679962965839
    },
    "mmlu_international_law": {
      "alias": "  - international_law",
      "acc,none": 0.24793388429752067,
      "acc_stderr,none": 0.03941897526516304
    },
    "mmlu_jurisprudence": {
      "alias": "  - jurisprudence",
      "acc,none": 0.19444444444444445,
      "acc_stderr,none": 0.038260763248848646
    },
    "mmlu_logical_fallacies": {
      "alias": "  - logical_fallacies",
      "acc,none": 0.3067484662576687,
      "acc_stderr,none": 0.036230899157241474
    },
    "mmlu_moral_disputes": {
      "alias": "  - moral_disputes",
      "acc,none": 0.2398843930635838,
      "acc_stderr,none": 0.02298959254312351
    },
    "mmlu_moral_scenarios": {
      "alias": "  - moral_scenarios",
      "acc,none": 0.23128491620111732,
      "acc_stderr,none": 0.01410222362315264
    },
    "mmlu_philosophy": {
      "alias": "  - philosophy",
      "acc,none": 0.2797427652733119,
      "acc_stderr,none": 0.02549425935069486
    },
    "mmlu_prehistory": {
      "alias": "  - prehistory",
      "acc,none": 0.25617283950617287,
      "acc_stderr,none": 0.02428853363772609
    },
    "mmlu_professional_law": {
      "alias": "  - professional_law",
      "acc,none": 0.2470664928292047,
      "acc_stderr,none": 0.011015752255279445
    },
    "mmlu_world_religions": {
      "alias": "  - world_religions",
      "acc,none": 0.3216374269005848,
      "acc_stderr,none": 0.03582529442573121
    },
    "mmlu_other": {
      "alias": " - other",
      "acc,none": 0.27518506598004505,
      "acc_stderr,none": 0.00795090160890387
    },
    "mmlu_business_ethics": {
      "alias": "  - business_ethics",
      "acc,none": 0.32,
      "acc_stderr,none": 0.04688261722621507
    },
    "mmlu_clinical_knowledge": {
      "alias": "  - clinical_knowledge",
      "acc,none": 0.21132075471698114,
      "acc_stderr,none": 0.0251257664848279
    },
    "mmlu_college_medicine": {
      "alias": "  - college_medicine",
      "acc,none": 0.2254335260115607,
      "acc_stderr,none": 0.03186209851641147
    },
    "mmlu_global_facts": {
      "alias": "  - global_facts",
      "acc,none": 0.21,
      "acc_stderr,none": 0.040936018074033236
    },
    "mmlu_human_aging": {
      "alias": "  - human_aging",
      "acc,none": 0.31390134529147984,
      "acc_stderr,none": 0.031146796482972486
    },
    "mmlu_management": {
      "alias": "  - management",
      "acc,none": 0.2815533980582524,
      "acc_stderr,none": 0.044532548363264673
    },
    "mmlu_marketing": {
      "alias": "  - marketing",
      "acc,none": 0.3076923076923077,
      "acc_stderr,none": 0.030236389942173154
    },
    "mmlu_medical_genetics": {
      "alias": "  - medical_genetics",
      "acc,none": 0.19,
      "acc_stderr,none": 0.039427724440366255
    },
    "mmlu_miscellaneous": {
      "alias": "  - miscellaneous",
      "acc,none": 0.2656449553001277,
      "acc_stderr,none": 0.015794302487888694
    },
    "mmlu_nutrition": {
      "alias": "  - nutrition",
      "acc,none": 0.23202614379084968,
      "acc_stderr,none": 0.024170840879340925
    },
    "mmlu_professional_accounting": {
      "alias": "  - professional_accounting",
      "acc,none": 0.25886524822695034,
      "acc_stderr,none": 0.026129572527180806
    },
    "mmlu_professional_medicine": {
      "alias": "  - professional_medicine",
      "acc,none": 0.44485294117647056,
      "acc_stderr,none": 0.030187532060329314
    },
    "mmlu_virology": {
      "alias": "  - virology",
      "acc,none": 0.26506024096385544,
      "acc_stderr,none": 0.034360240379449694
    },
    "mmlu_social_sciences": {
      "alias": " - social_sciences",
      "acc,none": 0.2577185570360741,
      "acc_stderr,none": 0.007873254291460508
    },
    "mmlu_econometrics": {
      "alias": "  - econometrics",
      "acc,none": 0.22807017543859648,
      "acc_stderr,none": 0.039471527826694136
    },
    "mmlu_high_school_geography": {
      "alias": "  - high_school_geography",
      "acc,none": 0.23737373737373738,
      "acc_stderr,none": 0.030313710538198924
    },
    "mmlu_high_school_government_and_politics": {
      "alias": "  - high_school_government_and_politics",
      "acc,none": 0.29533678756476683,
      "acc_stderr,none": 0.032922966391551386
    },
    "mmlu_high_school_macroeconomics": {
      "alias": "  - high_school_macroeconomics",
      "acc,none": 0.30512820512820515,
      "acc_stderr,none": 0.02334633529332588
    },
    "mmlu_high_school_microeconomics": {
      "alias": "  - high_school_microeconomics",
      "acc,none": 0.2689075630252101,
      "acc_stderr,none": 0.028801392193631228
    },
    "mmlu_high_school_psychology": {
      "alias": "  - high_school_psychology",
      "acc,none": 0.21651376146788992,
      "acc_stderr,none": 0.017658710594443204
    },
    "mmlu_human_sexuality": {
      "alias": "  - human_sexuality",
      "acc,none": 0.2824427480916031,
      "acc_stderr,none": 0.039484061257683584
    },
    "mmlu_professional_psychology": {
      "alias": "  - professional_psychology",
      "acc,none": 0.26633986928104575,
      "acc_stderr,none": 0.01788318813466726
    },
    "mmlu_public_relations": {
      "alias": "  - public_relations",
      "acc,none": 0.15454545454545454,
      "acc_stderr,none": 0.034622625712626656
    },
    "mmlu_security_studies": {
      "alias": "  - security_studies",
      "acc,none": 0.24489795918367346,
      "acc_stderr,none": 0.027529637440174917
    },
    "mmlu_sociology": {
      "alias": "  - sociology",
      "acc,none": 0.263681592039801,
      "acc_stderr,none": 0.03115715086935557
    },
    "mmlu_us_foreign_policy": {
      "alias": "  - us_foreign_policy",
      "acc,none": 0.32,
      "acc_stderr,none": 0.04688261722621507
    },
    "mmlu_stem": {
      "alias": " - stem",
      "acc,none": 0.2603869330796067,
      "acc_stderr,none": 0.007799817679078215
    },
    "mmlu_abstract_algebra": {
      "alias": "  - abstract_algebra",
      "acc,none": 0.25,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_anatomy": {
      "alias": "  - anatomy",
      "acc,none": 0.21481481481481482,
      "acc_stderr,none": 0.03547854198560821
    },
    "mmlu_astronomy": {
      "alias": "  - astronomy",
      "acc,none": 0.19078947368421054,
      "acc_stderr,none": 0.03197565821032503
    },
    "mmlu_college_biology": {
      "alias": "  - college_biology",
      "acc,none": 0.2361111111111111,
      "acc_stderr,none": 0.035514466108108246
    },
    "mmlu_college_chemistry": {
      "alias": "  - college_chemistry",
      "acc,none": 0.27,
      "acc_stderr,none": 0.04461960433384737
    },
    "mmlu_college_computer_science": {
      "alias": "  - college_computer_science",
      "acc,none": 0.31,
      "acc_stderr,none": 0.04648231987117317
    },
    "mmlu_college_mathematics": {
      "alias": "  - college_mathematics",
      "acc,none": 0.3,
      "acc_stderr,none": 0.04605661864718382
    },
    "mmlu_college_physics": {
      "alias": "  - college_physics",
      "acc,none": 0.28431372549019607,
      "acc_stderr,none": 0.04488482852329018
    },
    "mmlu_computer_security": {
      "alias": "  - computer_security",
      "acc,none": 0.32,
      "acc_stderr,none": 0.04688261722621507
    },
    "mmlu_conceptual_physics": {
      "alias": "  - conceptual_physics",
      "acc,none": 0.28936170212765955,
      "acc_stderr,none": 0.029644006577009652
    },
    "mmlu_electrical_engineering": {
      "alias": "  - electrical_engineering",
      "acc,none": 0.2413793103448276,
      "acc_stderr,none": 0.035659981741353035
    },
    "mmlu_elementary_mathematics": {
      "alias": "  - elementary_mathematics",
      "acc,none": 0.24338624338624337,
      "acc_stderr,none": 0.022101128787415353
    },
    "mmlu_high_school_biology": {
      "alias": "  - high_school_biology",
      "acc,none": 0.3064516129032258,
      "acc_stderr,none": 0.026226485652553904
    },
    "mmlu_high_school_chemistry": {
      "alias": "  - high_school_chemistry",
      "acc,none": 0.18719211822660098,
      "acc_stderr,none": 0.02744492496688264
    },
    "mmlu_high_school_computer_science": {
      "alias": "  - high_school_computer_science",
      "acc,none": 0.23,
      "acc_stderr,none": 0.04229525846816507
    },
    "mmlu_high_school_mathematics": {
      "alias": "  - high_school_mathematics",
      "acc,none": 0.2518518518518518,
      "acc_stderr,none": 0.026466117538959884
    },
    "mmlu_high_school_physics": {
      "alias": "  - high_school_physics",
      "acc,none": 0.2185430463576159,
      "acc_stderr,none": 0.03374235550425694
    },
    "mmlu_high_school_statistics": {
      "alias": "  - high_school_statistics",
      "acc,none": 0.3472222222222222,
      "acc_stderr,none": 0.03246887243637655
    },
    "mmlu_machine_learning": {
      "alias": "  - machine_learning",
      "acc,none": 0.25,
      "acc_stderr,none": 0.04109974682633932
    }
  }
}