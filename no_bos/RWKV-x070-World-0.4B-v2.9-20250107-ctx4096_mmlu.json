{
  "model": "/home/zhangping/zrc/rwkvmodels/RWKV-x070-World-0.4B-v2.9-20250107-ctx4096",
  "tasks": [
    "mmlu"
  ],
  "num_fewshot": 5,
  "results": {
    "mmlu": {
      "acc,none": 0.25851018373451073,
      "acc_stderr,none": 0.0036751039664625514,
      "alias": "mmlu"
    },
    "mmlu_humanities": {
      "alias": " - humanities",
      "acc,none": 0.2412327311370882,
      "acc_stderr,none": 0.006235970085635987
    },
    "mmlu_formal_logic": {
      "alias": "  - formal_logic",
      "acc,none": 0.19047619047619047,
      "acc_stderr,none": 0.03512207412302052
    },
    "mmlu_high_school_european_history": {
      "alias": "  - high_school_european_history",
      "acc,none": 0.2606060606060606,
      "acc_stderr,none": 0.03427743175816526
    },
    "mmlu_high_school_us_history": {
      "alias": "  - high_school_us_history",
      "acc,none": 0.24019607843137256,
      "acc_stderr,none": 0.029983733055913633
    },
    "mmlu_high_school_world_history": {
      "alias": "  - high_school_world_history",
      "acc,none": 0.21940928270042195,
      "acc_stderr,none": 0.026939106581553945
    },
    "mmlu_international_law": {
      "alias": "  - international_law",
      "acc,none": 0.2231404958677686,
      "acc_stderr,none": 0.038007544752287334
    },
    "mmlu_jurisprudence": {
      "alias": "  - jurisprudence",
      "acc,none": 0.21296296296296297,
      "acc_stderr,none": 0.039578354719809784
    },
    "mmlu_logical_fallacies": {
      "alias": "  - logical_fallacies",
      "acc,none": 0.24539877300613497,
      "acc_stderr,none": 0.033809398139433525
    },
    "mmlu_moral_disputes": {
      "alias": "  - moral_disputes",
      "acc,none": 0.24277456647398843,
      "acc_stderr,none": 0.02308365858698422
    },
    "mmlu_moral_scenarios": {
      "alias": "  - moral_scenarios",
      "acc,none": 0.2446927374301676,
      "acc_stderr,none": 0.014378169884098393
    },
    "mmlu_philosophy": {
      "alias": "  - philosophy",
      "acc,none": 0.21543408360128619,
      "acc_stderr,none": 0.023350225475471442
    },
    "mmlu_prehistory": {
      "alias": "  - prehistory",
      "acc,none": 0.3055555555555556,
      "acc_stderr,none": 0.025630824975621306
    },
    "mmlu_professional_law": {
      "alias": "  - professional_law",
      "acc,none": 0.2333767926988266,
      "acc_stderr,none": 0.010803108481179031
    },
    "mmlu_world_religions": {
      "alias": "  - world_religions",
      "acc,none": 0.29239766081871343,
      "acc_stderr,none": 0.03488647713457921
    },
    "mmlu_other": {
      "alias": " - other",
      "acc,none": 0.25329900225297713,
      "acc_stderr,none": 0.007756469808077499
    },
    "mmlu_business_ethics": {
      "alias": "  - business_ethics",
      "acc,none": 0.2,
      "acc_stderr,none": 0.04020151261036849
    },
    "mmlu_clinical_knowledge": {
      "alias": "  - clinical_knowledge",
      "acc,none": 0.26037735849056604,
      "acc_stderr,none": 0.027008766090708045
    },
    "mmlu_college_medicine": {
      "alias": "  - college_medicine",
      "acc,none": 0.28901734104046245,
      "acc_stderr,none": 0.03456425745086997
    },
    "mmlu_global_facts": {
      "alias": "  - global_facts",
      "acc,none": 0.25,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_human_aging": {
      "alias": "  - human_aging",
      "acc,none": 0.18834080717488788,
      "acc_stderr,none": 0.02624113299640725
    },
    "mmlu_management": {
      "alias": "  - management",
      "acc,none": 0.24271844660194175,
      "acc_stderr,none": 0.04245022486384496
    },
    "mmlu_marketing": {
      "alias": "  - marketing",
      "acc,none": 0.19230769230769232,
      "acc_stderr,none": 0.025819233256483692
    },
    "mmlu_medical_genetics": {
      "alias": "  - medical_genetics",
      "acc,none": 0.26,
      "acc_stderr,none": 0.0440844002276808
    },
    "mmlu_miscellaneous": {
      "alias": "  - miscellaneous",
      "acc,none": 0.2413793103448276,
      "acc_stderr,none": 0.015302380123542047
    },
    "mmlu_nutrition": {
      "alias": "  - nutrition",
      "acc,none": 0.24836601307189543,
      "acc_stderr,none": 0.02473998135511354
    },
    "mmlu_professional_accounting": {
      "alias": "  - professional_accounting",
      "acc,none": 0.2624113475177305,
      "acc_stderr,none": 0.02624492034984306
    },
    "mmlu_professional_medicine": {
      "alias": "  - professional_medicine",
      "acc,none": 0.40808823529411764,
      "acc_stderr,none": 0.02985526139348388
    },
    "mmlu_virology": {
      "alias": "  - virology",
      "acc,none": 0.21084337349397592,
      "acc_stderr,none": 0.03175554786629924
    },
    "mmlu_social_sciences": {
      "alias": " - social_sciences",
      "acc,none": 0.2677933051673708,
      "acc_stderr,none": 0.007942114950569505
    },
    "mmlu_econometrics": {
      "alias": "  - econometrics",
      "acc,none": 0.20175438596491227,
      "acc_stderr,none": 0.03775205013583637
    },
    "mmlu_high_school_geography": {
      "alias": "  - high_school_geography",
      "acc,none": 0.3434343434343434,
      "acc_stderr,none": 0.03383201223244441
    },
    "mmlu_high_school_government_and_politics": {
      "alias": "  - high_school_government_and_politics",
      "acc,none": 0.3626943005181347,
      "acc_stderr,none": 0.034697137917043715
    },
    "mmlu_high_school_macroeconomics": {
      "alias": "  - high_school_macroeconomics",
      "acc,none": 0.24615384615384617,
      "acc_stderr,none": 0.02184086699042312
    },
    "mmlu_high_school_microeconomics": {
      "alias": "  - high_school_microeconomics",
      "acc,none": 0.28991596638655465,
      "acc_stderr,none": 0.0294724858331361
    },
    "mmlu_high_school_psychology": {
      "alias": "  - high_school_psychology",
      "acc,none": 0.25504587155963304,
      "acc_stderr,none": 0.018688500856535898
    },
    "mmlu_human_sexuality": {
      "alias": "  - human_sexuality",
      "acc,none": 0.22137404580152673,
      "acc_stderr,none": 0.03641297081313727
    },
    "mmlu_professional_psychology": {
      "alias": "  - professional_psychology",
      "acc,none": 0.22875816993464052,
      "acc_stderr,none": 0.016992723465466174
    },
    "mmlu_public_relations": {
      "alias": "  - public_relations",
      "acc,none": 0.22727272727272727,
      "acc_stderr,none": 0.040139645540727735
    },
    "mmlu_security_studies": {
      "alias": "  - security_studies",
      "acc,none": 0.37551020408163266,
      "acc_stderr,none": 0.031001209039894864
    },
    "mmlu_sociology": {
      "alias": "  - sociology",
      "acc,none": 0.263681592039801,
      "acc_stderr,none": 0.03115715086935557
    },
    "mmlu_us_foreign_policy": {
      "alias": "  - us_foreign_policy",
      "acc,none": 0.2,
      "acc_stderr,none": 0.04020151261036849
    },
    "mmlu_stem": {
      "alias": " - stem",
      "acc,none": 0.28036790358388836,
      "acc_stderr,none": 0.007924668870888104
    },
    "mmlu_abstract_algebra": {
      "alias": "  - abstract_algebra",
      "acc,none": 0.29,
      "acc_stderr,none": 0.045604802157206865
    },
    "mmlu_anatomy": {
      "alias": "  - anatomy",
      "acc,none": 0.34074074074074073,
      "acc_stderr,none": 0.04094376269996792
    },
    "mmlu_astronomy": {
      "alias": "  - astronomy",
      "acc,none": 0.15789473684210525,
      "acc_stderr,none": 0.029674167520101446
    },
    "mmlu_college_biology": {
      "alias": "  - college_biology",
      "acc,none": 0.2777777777777778,
      "acc_stderr,none": 0.037455547914624555
    },
    "mmlu_college_chemistry": {
      "alias": "  - college_chemistry",
      "acc,none": 0.22,
      "acc_stderr,none": 0.041633319989322654
    },
    "mmlu_college_computer_science": {
      "alias": "  - college_computer_science",
      "acc,none": 0.33,
      "acc_stderr,none": 0.04725815626252609
    },
    "mmlu_college_mathematics": {
      "alias": "  - college_mathematics",
      "acc,none": 0.31,
      "acc_stderr,none": 0.04648231987117317
    },
    "mmlu_college_physics": {
      "alias": "  - college_physics",
      "acc,none": 0.20588235294117646,
      "acc_stderr,none": 0.040233822736177476
    },
    "mmlu_computer_security": {
      "alias": "  - computer_security",
      "acc,none": 0.16,
      "acc_stderr,none": 0.03684529491774706
    },
    "mmlu_conceptual_physics": {
      "alias": "  - conceptual_physics",
      "acc,none": 0.2297872340425532,
      "acc_stderr,none": 0.0275017529444124
    },
    "mmlu_electrical_engineering": {
      "alias": "  - electrical_engineering",
      "acc,none": 0.2896551724137931,
      "acc_stderr,none": 0.03780019230438011
    },
    "mmlu_elementary_mathematics": {
      "alias": "  - elementary_mathematics",
      "acc,none": 0.28835978835978837,
      "acc_stderr,none": 0.023330654054535868
    },
    "mmlu_high_school_biology": {
      "alias": "  - high_school_biology",
      "acc,none": 0.2903225806451613,
      "acc_stderr,none": 0.02582210611941596
    },
    "mmlu_high_school_chemistry": {
      "alias": "  - high_school_chemistry",
      "acc,none": 0.270935960591133,
      "acc_stderr,none": 0.03127090713297701
    },
    "mmlu_high_school_computer_science": {
      "alias": "  - high_school_computer_science",
      "acc,none": 0.21,
      "acc_stderr,none": 0.040936018074033236
    },
    "mmlu_high_school_mathematics": {
      "alias": "  - high_school_mathematics",
      "acc,none": 0.29259259259259257,
      "acc_stderr,none": 0.027738969632176085
    },
    "mmlu_high_school_physics": {
      "alias": "  - high_school_physics",
      "acc,none": 0.2980132450331126,
      "acc_stderr,none": 0.03734535676787201
    },
    "mmlu_high_school_statistics": {
      "alias": "  - high_school_statistics",
      "acc,none": 0.47685185185185186,
      "acc_stderr,none": 0.034063153607115024
    },
    "mmlu_machine_learning": {
      "alias": "  - machine_learning",
      "acc,none": 0.21428571428571427,
      "acc_stderr,none": 0.0389464112004479
    }
  }
}