{
  "model": "/home/zhangping/zrc/rwkvmodels/RWKV-x070-World-1.5B-v3-20250127-ctx4096",
  "tasks": [
    "mmlu"
  ],
  "num_fewshot": 5,
  "results": {
    "mmlu": {
      "acc,none": 0.4388263780088307,
      "acc_stderr,none": 0.004086287876006588,
      "alias": "mmlu"
    },
    "mmlu_humanities": {
      "alias": " - humanities",
      "acc,none": 0.40042507970244423,
      "acc_stderr,none": 0.006946376071058652
    },
    "mmlu_formal_logic": {
      "alias": "  - formal_logic",
      "acc,none": 0.30952380952380953,
      "acc_stderr,none": 0.04134913018303316
    },
    "mmlu_high_school_european_history": {
      "alias": "  - high_school_european_history",
      "acc,none": 0.5696969696969697,
      "acc_stderr,none": 0.03866225962879074
    },
    "mmlu_high_school_us_history": {
      "alias": "  - high_school_us_history",
      "acc,none": 0.5245098039215687,
      "acc_stderr,none": 0.035050931943487976
    },
    "mmlu_high_school_world_history": {
      "alias": "  - high_school_world_history",
      "acc,none": 0.4810126582278481,
      "acc_stderr,none": 0.03252375148090457
    },
    "mmlu_international_law": {
      "alias": "  - international_law",
      "acc,none": 0.48760330578512395,
      "acc_stderr,none": 0.04562951548180765
    },
    "mmlu_jurisprudence": {
      "alias": "  - jurisprudence",
      "acc,none": 0.4537037037037037,
      "acc_stderr,none": 0.04812917324536823
    },
    "mmlu_logical_fallacies": {
      "alias": "  - logical_fallacies",
      "acc,none": 0.4294478527607362,
      "acc_stderr,none": 0.038890666191127236
    },
    "mmlu_moral_disputes": {
      "alias": "  - moral_disputes",
      "acc,none": 0.48265895953757226,
      "acc_stderr,none": 0.0269029004586667
    },
    "mmlu_moral_scenarios": {
      "alias": "  - moral_scenarios",
      "acc,none": 0.26927374301675977,
      "acc_stderr,none": 0.014835616582882544
    },
    "mmlu_philosophy": {
      "alias": "  - philosophy",
      "acc,none": 0.5273311897106109,
      "acc_stderr,none": 0.028355633568328164
    },
    "mmlu_prehistory": {
      "alias": "  - prehistory",
      "acc,none": 0.5246913580246914,
      "acc_stderr,none": 0.027786800931427418
    },
    "mmlu_professional_law": {
      "alias": "  - professional_law",
      "acc,none": 0.3200782268578879,
      "acc_stderr,none": 0.011914791947638488
    },
    "mmlu_world_religions": {
      "alias": "  - world_religions",
      "acc,none": 0.695906432748538,
      "acc_stderr,none": 0.03528211258245229
    },
    "mmlu_other": {
      "alias": " - other",
      "acc,none": 0.4895397489539749,
      "acc_stderr,none": 0.008801062212456776
    },
    "mmlu_business_ethics": {
      "alias": "  - business_ethics",
      "acc,none": 0.33,
      "acc_stderr,none": 0.04725815626252609
    },
    "mmlu_clinical_knowledge": {
      "alias": "  - clinical_knowledge",
      "acc,none": 0.5018867924528302,
      "acc_stderr,none": 0.030772653642075643
    },
    "mmlu_college_medicine": {
      "alias": "  - college_medicine",
      "acc,none": 0.43352601156069365,
      "acc_stderr,none": 0.03778621079092051
    },
    "mmlu_global_facts": {
      "alias": "  - global_facts",
      "acc,none": 0.31,
      "acc_stderr,none": 0.04648231987117317
    },
    "mmlu_human_aging": {
      "alias": "  - human_aging",
      "acc,none": 0.5112107623318386,
      "acc_stderr,none": 0.0335493665309847
    },
    "mmlu_management": {
      "alias": "  - management",
      "acc,none": 0.5825242718446602,
      "acc_stderr,none": 0.04882840548212234
    },
    "mmlu_marketing": {
      "alias": "  - marketing",
      "acc,none": 0.6196581196581197,
      "acc_stderr,none": 0.031804252043841054
    },
    "mmlu_medical_genetics": {
      "alias": "  - medical_genetics",
      "acc,none": 0.51,
      "acc_stderr,none": 0.05024183937956913
    },
    "mmlu_miscellaneous": {
      "alias": "  - miscellaneous",
      "acc,none": 0.5772669220945083,
      "acc_stderr,none": 0.01766518035195396
    },
    "mmlu_nutrition": {
      "alias": "  - nutrition",
      "acc,none": 0.5392156862745098,
      "acc_stderr,none": 0.028541722692618926
    },
    "mmlu_professional_accounting": {
      "alias": "  - professional_accounting",
      "acc,none": 0.30141843971631205,
      "acc_stderr,none": 0.027374128882631205
    },
    "mmlu_professional_medicine": {
      "alias": "  - professional_medicine",
      "acc,none": 0.43014705882352944,
      "acc_stderr,none": 0.030074971917302858
    },
    "mmlu_virology": {
      "alias": "  - virology",
      "acc,none": 0.3614457831325301,
      "acc_stderr,none": 0.03740059382029325
    },
    "mmlu_social_sciences": {
      "alias": " - social_sciences",
      "acc,none": 0.4946376340591485,
      "acc_stderr,none": 0.008866166124284407
    },
    "mmlu_econometrics": {
      "alias": "  - econometrics",
      "acc,none": 0.2543859649122807,
      "acc_stderr,none": 0.04096985139843672
    },
    "mmlu_high_school_geography": {
      "alias": "  - high_school_geography",
      "acc,none": 0.5151515151515151,
      "acc_stderr,none": 0.03560716516531066
    },
    "mmlu_high_school_government_and_politics": {
      "alias": "  - high_school_government_and_politics",
      "acc,none": 0.616580310880829,
      "acc_stderr,none": 0.03508984236295346
    },
    "mmlu_high_school_macroeconomics": {
      "alias": "  - high_school_macroeconomics",
      "acc,none": 0.45384615384615384,
      "acc_stderr,none": 0.02524277098712623
    },
    "mmlu_high_school_microeconomics": {
      "alias": "  - high_school_microeconomics",
      "acc,none": 0.41596638655462187,
      "acc_stderr,none": 0.03201650100739608
    },
    "mmlu_high_school_psychology": {
      "alias": "  - high_school_psychology",
      "acc,none": 0.5981651376146789,
      "acc_stderr,none": 0.021020106172996954
    },
    "mmlu_human_sexuality": {
      "alias": "  - human_sexuality",
      "acc,none": 0.48854961832061067,
      "acc_stderr,none": 0.043841400240780176
    },
    "mmlu_professional_psychology": {
      "alias": "  - professional_psychology",
      "acc,none": 0.4444444444444444,
      "acc_stderr,none": 0.020102583895887222
    },
    "mmlu_public_relations": {
      "alias": "  - public_relations",
      "acc,none": 0.42727272727272725,
      "acc_stderr,none": 0.04738198703545482
    },
    "mmlu_security_studies": {
      "alias": "  - security_studies",
      "acc,none": 0.39591836734693875,
      "acc_stderr,none": 0.03130802899065682
    },
    "mmlu_sociology": {
      "alias": "  - sociology",
      "acc,none": 0.6517412935323383,
      "acc_stderr,none": 0.033687874661154624
    },
    "mmlu_us_foreign_policy": {
      "alias": "  - us_foreign_policy",
      "acc,none": 0.59,
      "acc_stderr,none": 0.04943110704237104
    },
    "mmlu_stem": {
      "alias": " - stem",
      "acc,none": 0.3916904535363146,
      "acc_stderr,none": 0.00858241481853151
    },
    "mmlu_abstract_algebra": {
      "alias": "  - abstract_algebra",
      "acc,none": 0.33,
      "acc_stderr,none": 0.04725815626252609
    },
    "mmlu_anatomy": {
      "alias": "  - anatomy",
      "acc,none": 0.4444444444444444,
      "acc_stderr,none": 0.04292596718256977
    },
    "mmlu_astronomy": {
      "alias": "  - astronomy",
      "acc,none": 0.4934210526315789,
      "acc_stderr,none": 0.040685900502249725
    },
    "mmlu_college_biology": {
      "alias": "  - college_biology",
      "acc,none": 0.4444444444444444,
      "acc_stderr,none": 0.04155319955593143
    },
    "mmlu_college_chemistry": {
      "alias": "  - college_chemistry",
      "acc,none": 0.38,
      "acc_stderr,none": 0.04878317312145634
    },
    "mmlu_college_computer_science": {
      "alias": "  - college_computer_science",
      "acc,none": 0.39,
      "acc_stderr,none": 0.04902071300001973
    },
    "mmlu_college_mathematics": {
      "alias": "  - college_mathematics",
      "acc,none": 0.38,
      "acc_stderr,none": 0.04878317312145634
    },
    "mmlu_college_physics": {
      "alias": "  - college_physics",
      "acc,none": 0.3431372549019608,
      "acc_stderr,none": 0.04724007352383884
    },
    "mmlu_computer_security": {
      "alias": "  - computer_security",
      "acc,none": 0.6,
      "acc_stderr,none": 0.0492365963917331
    },
    "mmlu_conceptual_physics": {
      "alias": "  - conceptual_physics",
      "acc,none": 0.3702127659574468,
      "acc_stderr,none": 0.031565646822367815
    },
    "mmlu_electrical_engineering": {
      "alias": "  - electrical_engineering",
      "acc,none": 0.45517241379310347,
      "acc_stderr,none": 0.04149886942192114
    },
    "mmlu_elementary_mathematics": {
      "alias": "  - elementary_mathematics",
      "acc,none": 0.26455026455026454,
      "acc_stderr,none": 0.022717467897708593
    },
    "mmlu_high_school_biology": {
      "alias": "  - high_school_biology",
      "acc,none": 0.5129032258064516,
      "acc_stderr,none": 0.028434533152681855
    },
    "mmlu_high_school_chemistry": {
      "alias": "  - high_school_chemistry",
      "acc,none": 0.3694581280788177,
      "acc_stderr,none": 0.03395970381998578
    },
    "mmlu_high_school_computer_science": {
      "alias": "  - high_school_computer_science",
      "acc,none": 0.36,
      "acc_stderr,none": 0.048241815132442176
    },
    "mmlu_high_school_mathematics": {
      "alias": "  - high_school_mathematics",
      "acc,none": 0.2777777777777778,
      "acc_stderr,none": 0.027309140588230203
    },
    "mmlu_high_school_physics": {
      "alias": "  - high_school_physics",
      "acc,none": 0.3576158940397351,
      "acc_stderr,none": 0.0391345343117726
    },
    "mmlu_high_school_statistics": {
      "alias": "  - high_school_statistics",
      "acc,none": 0.4537037037037037,
      "acc_stderr,none": 0.033953227263757976
    },
    "mmlu_machine_learning": {
      "alias": "  - machine_learning",
      "acc,none": 0.38392857142857145,
      "acc_stderr,none": 0.04616143075028549
    }
  }
}